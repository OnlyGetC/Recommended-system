{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./images/DLI_Header.png\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternating Least Squares with Real Data\n",
    "\n",
    "Since we already have practice working with sparse data, we can apply the same techniques to our Amazon dataset. The trick is that because the data is so large, it will be trickier to use our debugging techniques from the <a href=\"1-05_als.ipynb\"></a>previous notebook.\n",
    "\n",
    "## Objectives\n",
    "This notebook demonstrates:\n",
    "* How to build a collaborative filter for large datasets\n",
    "    * [1. Splitting into Train and Test](#1.-Splitting-into-Train-and-Test)\n",
    "    * [2. Alternating Least Squares](#2.-Alternating-Least-Squares)\n",
    "    * [3. Wrap Up](#3.-Wrap-Up)\n",
    "\n",
    "## 1. Splitting into Train and Test    \n",
    "We already did data cleansing in an earlier notebook, so let's load it up along with our libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3V543DBS5C4RU</td>\n",
       "      <td>B0000510ZO</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A273KT0BQJCAXE</td>\n",
       "      <td>B0000510ZO</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1RYZ3WCLIEZHK</td>\n",
       "      <td>B0000510ZO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1IWH0NZGT65RM</td>\n",
       "      <td>B0000510ZO</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADE6RHHG0SWWM</td>\n",
       "      <td>B0000510ZO</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  overall  valid\n",
       "0  A3V543DBS5C4RU  B0000510ZO      5.0  False\n",
       "1  A273KT0BQJCAXE  B0000510ZO      5.0  False\n",
       "2  A1RYZ3WCLIEZHK  B0000510ZO      1.0  False\n",
       "3  A1IWH0NZGT65RM  B0000510ZO      5.0  False\n",
       "4   ADE6RHHG0SWWM  B0000510ZO      4.0  False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import cupyx.scipy.sparse\n",
    "import cudf\n",
    "import numpy as np\n",
    "\n",
    "ratings = cudf.read_csv(\"data/ratings.csv\")\n",
    "\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we split into train and test, let's [factorize](https://docs.rapids.ai/api/cudf/stable/api_docs/api/cudf.Series.factorize.html) our users and items. Our test set won't be effective if it doesn't use the same indexes as the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_indexes, user_mapping = ratings[\"reviewerID\"].factorize()\n",
    "item_indexes, item_mapping = ratings[\"asin\"].factorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're building our sparse matrix, let's get a sense of our data along the way. First, let's see how many users there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192403"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_count = user_mapping.count()\n",
    "\n",
    "user_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost two hundred thousand! Okay, how about the items?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_count = item_mapping.count()\n",
    "\n",
    "item_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much bigger than our toy problem with four users and five items. Using [todense](https://docs-cupy.chainer.org/en/stable/reference/generated/cupyx.scipy.sparse.coo_matrix.html#cupyx.scipy.sparse.coo_matrix.todense) will result in memory limitations, which in the real world is both frustrating and expensive.\n",
    "\n",
    "Instead, we'll need to rely on good testing. Let's break our ratings data down into train and test. Please fill in the `FIXME`s below, using the <a href=\"1-05_als_intro.ipynb\">previous lab</a> as a hint if needed. There is one `FIXME` for each input of the `get_dataset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "train_indexes = ~ratings[\"valid\"]\n",
    "valid_indexes = ratings[\"valid\"]\n",
    "overall = ratings[\"overall\"]\n",
    "\n",
    "def get_dataset(data_selector, user_indexes, item_indexes, overall):\n",
    "    # FIXME: find als rows and columns based on indexes\n",
    "    row = cp.asarray(user_indexes[data_selector])\n",
    "    column = cp.asarray(item_indexes[data_selector])\n",
    "    data = cp.asarray(overall[data_selector])\n",
    "    \n",
    "    # FIXME: build sparse matrix with correct shape\n",
    "    sparse_data = cupyx.scipy.sparse.coo_matrix((data, (row, column)))#, shape=FIXME_E\n",
    "    mask = cp.asarray([1 for _ in range(len(data))], dtype=np.float32)\n",
    "    sparse_mask = cupyx.scipy.sparse.coo_matrix((mask, (row, column)))#, shape=FIXME_E\n",
    "    return row, column, data, sparse_data, sparse_mask\n",
    "\n",
    "train_row, train_column, train_data, train_sparse, train_mask = get_dataset(\n",
    "    train_indexes, user_indexes, item_indexes, overall)\n",
    "\n",
    "valid_row, valid_column, valid_data, valid_sparse, valid_mask = get_dataset(\n",
    "    valid_indexes, user_indexes, item_indexes, overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Alternating Least Squares\n",
    "\n",
    "It's time to initialize our embeddings. Let's copy over our functions from the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = 2\n",
    "shape = (user_count, item_count)\n",
    "\n",
    "def initalize_features(length, embeddings):\n",
    "    return cp.random.rand(embeddings, length) * 2 - 1\n",
    "\n",
    "user_features = initalize_features(shape[0], embeddings)\n",
    "item_features = initalize_features(shape[1], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(user_features, item_features, data, row, column):\n",
    "    predictions = (user_features[:, row] * item_features[:, column]).sum(axis=0)\n",
    "    mean_squared_error = ((predictions - data) ** 2).mean() ** 0.5\n",
    "    \n",
    "    return predictions, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ALS function below is incomplete. Please fill in the `FIXME`s to get it up and running. There is one `FIXME` for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def als(values, mask, features, scale=0.01):\n",
    "    # FIXME: add inputs to als algorithm\n",
    "    numerator = values.dot(features.T)\n",
    "    squared_features = (features ** 2).sum(axis=0)\n",
    "    denominator = scale + mask.dot(squared_features)\n",
    "\n",
    "    return (numerator / denominator[:, None]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How low can you go? Run the below cell multiple times to watch the RMSE fall. If the above `FIXME`s are correctly implemented, the RMSE can reach below `1.25`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FIXME_A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a8e191b00d84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0muser_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mitem_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     predictions, error = rmse(\n\u001b[1;32m      5\u001b[0m         user_features, item_features, valid_data, valid_row, valid_column)\n",
      "\u001b[0;32m<ipython-input-9-ed489bf04240>\u001b[0m in \u001b[0;36mals\u001b[0;34m(values, mask, features, scale)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# FIXME: add inputs to als algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnumerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFIXME_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFIXME_B\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msquared_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mFIXME_B\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFIXME_C\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mFIXME_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquared_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FIXME_A' is not defined"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    user_features = als(train_sparse, train_mask, item_features)\n",
    "    item_features = als(train_sparse.T, train_mask.T, user_features)\n",
    "    predictions, error = rmse(\n",
    "        user_features, item_features, valid_data, valid_row, valid_column)\n",
    "\n",
    "    print (\"RMSE:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a human eyeball test to verify our prediction error. Do the predictions follow the same ranking order as the true ratings do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wrap Up\n",
    "\n",
    "To save our work, let's group the notebooks in a zip file by running the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r lab1_work.zip . -i '1*.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the zip file, right click on it from the file menu to the left and select \"Download\". The file menu may need to be refreshed by clicking the arrow circle above the list.\n",
    "\n",
    "Congratulations on finishing this set of notebooks! We learned how to do two major types of Recommender Systems: Content-based and Collaborative. While we were able to achieve a lower RMSE with Collaborative Filtering, there are some situations were Content-based wins out. Have an idea what they are? Head back over to the notebook launcher to take a quiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./images/DLI_Header.png\"></a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
